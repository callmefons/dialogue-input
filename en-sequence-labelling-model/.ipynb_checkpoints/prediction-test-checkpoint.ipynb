{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    BATCH_SIZE = 512  # Number of examples used in each iteration\n",
    "    EPOCHS = 5  # Number of passes through entire dataset\n",
    "    MAX_LEN = 20  # Max length of review (in words)\n",
    "    EMBEDDING = 40  # Dimension of word embedding vector\n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 20\n",
    "    MAX_LEN = 75\n",
    "    EMBEDDING = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  1778\n",
      "Number of words in the dataset:  5838\n",
      "Tags: ['I-FREQUENCY', 'B-DRUG', 'B-FREQUENCY', 'I-ROUTE', 'O', 'B-STRENGTH', 'I-DRUG', 'I-DURATION', 'B-DURATION', 'B-ROUTE', 'B-FORM', 'B-DOSAGE', 'I-STRENGTH', 'I-DOSAGE']\n",
      "Number of Labels:  14\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/drug_bio_all.csv\", encoding=\"utf8\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "print(\"Number of sentences: \", len(data.groupby(['Sentence #'])))\n",
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words)\n",
    "print(\"Number of words in the dataset: \", n_words)\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "print(\"Tags:\", tags)\n",
    "n_tags = len(tags)\n",
    "print(\"Number of Labels: \", n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Slight', 'wean', 'of', 'Levo']]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Slight wean of Levo\"\n",
    "sentences = []\n",
    "\n",
    "doc = nlp(sentence)\n",
    "for token in doc:\n",
    "    sentences.append(token.text)\n",
    "\n",
    "sentences = [sentences]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "X = [[word2idx[w] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple sequential model\n",
    "def create_model():\n",
    "    # Model definition\n",
    "    input = Input(shape=(MAX_LEN,))\n",
    "    model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                      input_length=MAX_LEN, mask_zero=True)(input)  \n",
    "    model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                               recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "    model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "    crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "    out = crf(model)  # output\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tittaya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/Users/tittaya/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 75, 32)            186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 75, 100)           33200     \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 75, 50)            5050      \n",
      "_________________________________________________________________\n",
      "crf_13 (CRF)                 (None, 75, 15)            1020      \n",
      "=================================================================\n",
      "Total params: 226,150\n",
      "Trainable params: 226,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a new model instance\n",
    "model = create_model()\n",
    "\n",
    "# Restore the weights\n",
    "model.load_weights('./training/cp-0000.ckpt')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-DOSAGE', 'B-DOSAGE', 'I-DOSAGE', 'B-DOSAGE', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "pred_cat = model.predict(X[0:1])\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "print(pred_tag[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Slight', 'wean', 'of', 'Levo', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "text = [[idx2word[w] for w in s] for s in X]\n",
    "print(text[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'DOSAGE': 'Slight'},\n",
       " {'DOSAGE': 'wean'},\n",
       " {'DOSAGE': 'of'},\n",
       " {'DOSAGE': 'Levo'}]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_tokens = [\"UNK\", \"PAD\", \"O\"]\n",
    "labels = []\n",
    "is_begin = False\n",
    "\n",
    "for i in range(pred.shape[1]):\n",
    "    wordDicts = {}\n",
    "    bio_tag = \"\"\n",
    "    if pred_tag[0][i] not in ignore_tokens:\n",
    "        wordDicts[pred_tag[0][i].split(\"-\")[1]] = (text[0][i])\n",
    "        labels.append(wordDicts)       \n",
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
